{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we are going to train a custom Transformer model with using pytorch.  "
      ],
      "metadata": {
        "id": "h8RAFx2tpHBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n",
        "We will install and import all the required libraries"
      ],
      "metadata": {
        "id": "t1AQYU73pYk7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq5s7kGGo4aV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7181d15-138d-49cf-e9ac-0d0fdef0f714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.5/268.5 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets tokenizers --q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace"
      ],
      "metadata": {
        "id": "9F7g9gRGpnD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the availability of the cuda device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")  # Use CUDA device\n",
        "else:\n",
        "    device = torch.device(\"cpu\")  # Use CPU device"
      ],
      "metadata": {
        "id": "hHgNHYIRpyZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SX_1142prKvT",
        "outputId": "b3bedbc6-9c52-4fc5-a39a-2553af053dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"+\"Sjar\"+\" \""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7O7Bh6l637yT",
        "outputId": "940afbe1-fdee-4a40-9a89-1337066ae640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sjar '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing\n",
        "In this section firsly, we will read the text files which later, we will divide songs into train, test and validation set. Please note that we are making testing conditions hard by explicitingly spliting based on songs than joining all the text together and spliting the text.\n",
        "\n",
        "We will perform heuristic analysis on the training text in terms of:\n",
        "1. Average number of verses in a song\n",
        "2. Average number of STOPWORDs in a song\n",
        "\n"
      ],
      "metadata": {
        "id": "uCd6VXqpp6zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We are using special tokens [EOS] and [SEP] to denote the end of the song\n",
        "and end of the verse respectively\n",
        "\"\"\"\n",
        "\n",
        "album_path = \"/content/drive/MyDrive/Colab_Notebooks/Outsystems/data/Albums\"\n",
        "text_data = []\n",
        "for root, dirs, files in os.walk(album_path):\n",
        "    for name in files:\n",
        "        with open(os.path.join(root, name), mode=\"r\", encoding=\"utf-8\") as f:\n",
        "            lines = f.readlines()[1:]\n",
        "            # adding [EOS] at the end of each song\n",
        "            lines = \"\".join(lines)\n",
        "            ## adding [SEP] in between verses\n",
        "            # lines = re.sub(r\"\\n \\n\", \" [SEP]\", lines)\n",
        "            # lines = re.sub(r\"\\n\", \"\", lines)\n",
        "            # text_data += lines + \" \"\n",
        "            text_data.append(lines)\n",
        "            f.close()"
      ],
      "metadata": {
        "id": "QCanibPdq5ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install revtok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEEipKYPgjpW",
        "outputId": "0788baa2-2ca7-4e02-9fd0-88b6575e6555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting revtok\n",
            "  Downloading revtok-0.0.3-py3-none-any.whl (4.3 kB)\n",
            "Installing collected packages: revtok\n",
            "Successfully installed revtok-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_data[:10][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4w7Q4Mu4ON0",
        "outputId": "c2edea38-e2fd-4e40-ef19-af8a5b673a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was so nice throwing big parties\n",
            "Jump into the pool from the balcony\n",
            "Everyone swimming in a champagne sea\n",
            "And there are no rules when you show up here\n",
            "Bass beat rattling the chandelier\n",
            "Feeling so Gatsby for that whole year\n",
            "\n",
            "[Pre-Chorus]\n",
            "So, why'd you have to rain on my parade?\n",
            "I'm shaking my head and locking the gates\n",
            "[Chorus]\n",
            "This is why we can't have nice things, darling\n",
            "Because you break them, I had to take them away\n",
            "This is why we can't have nice things, honey (Oh)\n",
            "Did you think I wouldn't hear all the things you said about me?\n",
            "This is why we can't have nice things\n",
            "\n",
            "[Verse 2]\n",
            "It was so nice being friends again\n",
            "There I was, giving you a second chance\n",
            "But you stabbed me in the back while shaking my hand\n",
            "And therein lies the issue, friends don't try to trick you\n",
            "Get you on the phone and mind-twist you\n",
            "And so I took an axe to a mended fence\n",
            "\n",
            "[Pre-Chorus]\n",
            "But I'm not the only friend you've lost lately (Mm-mm)\n",
            "If only you weren't so shady\n",
            "[Chorus]\n",
            "This is why we can't have nice things, darling (Yeah)\n",
            "Because you break them, I had to take them away\n",
            "This is why we can't have nice (Nice things) things (Baby), honey\n",
            "Did you think I wouldn't hear all the things you said about me?\n",
            "This is why we can't have—\n",
            "You might also like[Bridge]\n",
            "Here's a toast to my real friends\n",
            "They don't care about the he-said, she-said\n",
            "And here's to my baby\n",
            "He ain't reading what they call me lately\n",
            "And here's to my mama\n",
            "Had to listen to all this drama\n",
            "And here's to you\n",
            "'Cause forgiveness is a nice thing to do\n",
            "Haha, I can't even say it with a straight face\n",
            "[Chorus]\n",
            "This is why we can't have nice things, darling (Darling)\n",
            "Because you break them, I had to take them away\n",
            "This is why we can't have nice (Uh-uh) things (Oh, no), honey (Baby, oh)\n",
            "Did you think I wouldn't hear all the things you said about me?\n",
            "This is why we can't have nice things, darling\n",
            "(And here's to my real friends)\n",
            "Because you break them, I had to take them\n",
            "(And here's to my baby)\n",
            "Nice things, honey\n",
            "(They didn't care about that he-said, she-said)\n",
            "Did you think I wouldn't hear all the things you said about me?\n",
            "This is why we can't have nice things55Embed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into 80-10-10 data split\n",
        "train, val = train_test_split(text_data, test_size=0.1, random_state=99)\n",
        "train, test = train_test_split(train, test_size=0.1, random_state=99)"
      ],
      "metadata": {
        "id": "2hzmG04KrsX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = \"\".join(train)\n",
        "val_text = \"\".join(val)\n",
        "test_text = \"\".join(test)"
      ],
      "metadata": {
        "id": "X_fO0-P76B9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer('subword')\n",
        "\n",
        "def yield_tokens(file_path):\n",
        "  for root, dirs, files in os.walk(file_path):\n",
        "    for name in files:\n",
        "      with io.open(os.path.join(root, name), encoding = 'utf-8') as f:\n",
        "        for line in f:\n",
        "          yield tokenizer(line.strip())\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(album_path), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "KH2jLpRR8pR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_process(raw_text_iter):\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item.strip())), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
      ],
      "metadata": {
        "id": "lwb7wJkiil70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer('subword')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, map(lambda x:x.strip(),train_iter)), max_tokens=15346)\n",
        "vocab.set_default_index(vocab[\"[UNK]\"])\n",
        "\n",
        "def data_process(raw_text_iter):\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item.strip())), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
      ],
      "metadata": {
        "id": "ujL_aM5Y6ow4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text analysis"
      ],
      "metadata": {
        "id": "Nj-C-AkIr2jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_verses = lambda song: len(song.split(\"[SEP]\"))\n",
        "stopwords = set([\n",
        "        \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\",\n",
        "        \"from\", \"has\", \"he\", \"in\", \"is\", \"it\", \"its\", \"of\", \"on\",\n",
        "        \"that\", \"the\", \"to\", \"was\", \"were\", \"will\", \"with\"\n",
        "    ])\n",
        "\n",
        "def count_stopwords(text, stopwords=stopwords):\n",
        "    word_list = text.lower().split()\n",
        "    stopwords_count = 0\n",
        "\n",
        "    for word in word_list:\n",
        "        if word in stopwords:\n",
        "            stopwords_count += 1\n",
        "\n",
        "    return stopwords_count\n",
        "\n",
        "avg_n_verses = sum([count_verses(t) for t in train]) / len(train)\n",
        "avg_n_words = sum([len(t.split()) for t in train]) / len(train)\n",
        "avg_stop_verses = sum([count_stopwords(t) for t in train]) / len(train)\n",
        "len_vocab = len(set([w for t in train for w in t.split()]))\n",
        "\n",
        "print(f\"Average number of verses in a song {avg_n_verses:.4f} \\n\")\n",
        "print(f\"Average number of words in a song {avg_n_words:.4f} \\n\")\n",
        "print(f\"Average number of stopwords in a song {avg_stop_verses:.4f} \\n\")\n",
        "print(f\"Average number of words in a verse {avg_n_words/avg_n_verses:.4f} \\n\")\n",
        "print(f\"Vocab size {len_vocab}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOyHi61Nr1aK",
        "outputId": "70ea4f16-5663-4620-8dac-c77dac958835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average number of verses in a song 1.0000 \n",
            "\n",
            "Average number of words in a song 474.4198 \n",
            "\n",
            "Average number of stopwords in a song 97.3309 \n",
            "\n",
            "Average number of words in a verse 474.4198 \n",
            "\n",
            "Vocab size 15344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The requirement of the task is to generate song of based on initial one or two verses. Due to memory constraints, we will consider only one **verse of length 64** (closest int to the power of 2 for 70.37) to generate maximum sequnece **length of song 512** (closest int to the power of 2 for 481.25). We will not remove the stopwords to achieve grammatically correct sentence, which will eventually reduce the perplexity of the generated sequence which would be the evaluation metric for the task."
      ],
      "metadata": {
        "id": "SIBmKSqSvtL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "class TaylorLyricsDataset(IterableDataset):\n",
        "    \"\"\"\n",
        "    A custom IterableDataset implementation.\n",
        "\n",
        "    This class allows iterating over the provided data by implementing the __iter__ method.\n",
        "    It inherits from the IterableDataset class.\n",
        "\n",
        "    Args:\n",
        "        data (Iterable): The data to be used for iteration.\n",
        "\n",
        "    Yields:\n",
        "        Any: Each item from the provided data.\n",
        "    \"\"\"\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __iter__(self):\n",
        "        for item in self.data:\n",
        "            # Yield or return each item from the data\n",
        "            yield item\n",
        "\n",
        "train_iter = TaylorLyricsDataset(train)\n",
        "test_iter = TaylorLyricsDataset(test)\n",
        "val_iter = TaylorLyricsDataset(val)"
      ],
      "metadata": {
        "id": "zZLCLXePuFci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "\n",
        "For this task, we are using basic English tokenizer from torchtext by taking small vocabulary size (15346) into account. As other tokenizers based on subword needs large vocabulary size to learn the frequency patterns of the characters appearing together."
      ],
      "metadata": {
        "id": "No8qTpF5xjkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install revtok -q"
      ],
      "metadata": {
        "id": "TjjHHvuAf3PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer"
      ],
      "metadata": {
        "id": "5pgCsOkDJLvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer('subword')\n",
        "# vocab = build_vocab_from_iterator([tokenizer(item) for item in train_iter])\n",
        "\n",
        "for idx, item in enumerate(list(train_iter)):\n",
        "  try:\n",
        "    tokenizer(item.strip())\n",
        "  except IndexError:\n",
        "    print(f\"Error at {idx}\")"
      ],
      "metadata": {
        "id": "dc_UTU4OgktI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(train_iter)[64].strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "67J3CmfGmBWJ",
        "outputId": "47319491-6835-4fd8-c116-4a05a6869f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[Verse 1] Once upon a time, a few mistakes ago I was in your sights, you got me alone You found me, you found me You found me-e-e-e-e I guess you didn't care, and I guess I liked that And when I fell hard, you took a step back Without me, without me Without me-e-e-e-e [Pre-Chorus] And he's long gone when he's next to me And I realize the blame is on me [SEP] [Chorus] 'Cause I knew you were trouble when you walked in So shame on me now Flew me to places I'd never been 'Til you put me down, oh I knew you were trouble when you walked in So shame on me now Flew me to places I'd never been Now, I'm lying on the cold, hard ground [SEP] [Post-Chorus] Oh, oh-oh Trouble, trouble, trouble Oh, oh-oh Trouble, trouble, trouble [SEP] [Verse 2] No apologies, he'll never see you cry Pretends he doesn't know that he's the reason why You're drowning, you're drowning You're drowning-ing-ing-ing-ing And I heard you moved on from whispers on the street A new notch in your belt is all I'll ever be And now, I see, now, I see Now, I see-e-e-e-e You might also like[Pre-Chorus] He was long gone when he met me And I realize the joke is on me, yeah [SEP] [Chorus] I knew you were trouble when you walked in (Oh) So shame on me now Flew me to places I'd never been 'Til you put me down, oh I knew you were trouble when you walked in So shame on me now Flew me to places I'd never been (Yeah) Now, I'm lying on the cold, hard ground [SEP] [Post-Chorus] Oh, oh-oh Trouble, trouble, trouble (Yeah, trouble) Oh, oh-oh Trouble, trouble, trouble [SEP] [Bridge] And the saddest fear Comes creeping in That you never loved me Or her, or anyone, or anything, yeah [Chorus] I knew you were trouble when you walked in So shame on me now Flew me to places I'd never been (Never been) 'Til you put me down, oh I knew you were trouble when you walked in (Knew it right there) So shame on me now (Knew it right there) Flew me to places I'd never been (Ooh) Now, I'm lying on the cold, hard ground [SEP] [Post-Chorus] Oh, oh-oh Trouble, trouble, trouble (Oh) Oh, oh-oh Trouble, trouble, trouble I knew you were trouble when you walked in Trouble, trouble, trouble I knew you were trouble when you walked in Trouble, trouble, trouble [Outro]112Embed [EOS]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(list(train_iter)[64].strip())"
      ],
      "metadata": {
        "id": "J8DSUwWLJmpq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81cc3fb1-d31a-4b4f-d223-8df6b92bc97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' [',\n",
              " '\\ue302 verse ',\n",
              " ' 1 ',\n",
              " '] ',\n",
              " '\\ue302 once ',\n",
              " ' upon ',\n",
              " ' a ',\n",
              " ' time ',\n",
              " ', ',\n",
              " ' a ',\n",
              " ' few ',\n",
              " ' mistakes ',\n",
              " ' ago ',\n",
              " ' I ',\n",
              " ' was ',\n",
              " ' in ',\n",
              " ' your ',\n",
              " ' sights ',\n",
              " ', ',\n",
              " ' you ',\n",
              " ' got ',\n",
              " ' me ',\n",
              " ' alone ',\n",
              " '\\ue302 you ',\n",
              " ' found ',\n",
              " ' me ',\n",
              " ', ',\n",
              " ' you ',\n",
              " ' found ',\n",
              " ' me ',\n",
              " '\\ue302 you ',\n",
              " ' found ',\n",
              " ' me ',\n",
              " '-',\n",
              " ' e ',\n",
              " '-',\n",
              " ' e ',\n",
              " '-',\n",
              " ' e ',\n",
              " '-',\n",
              " ' e ',\n",
              " ' I ',\n",
              " ' guess ',\n",
              " ' you ',\n",
              " ' didn ',\n",
              " \"'\",\n",
              " ' t ',\n",
              " ' care ',\n",
              " ', ',\n",
              " ' and ',\n",
              " ' I ',\n",
              " ' guess ',\n",
              " ' I ',\n",
              " ' liked ',\n",
              " ' that ',\n",
              " '\\ue302 and ',\n",
              " ' when ',\n",
              " ' I ',\n",
              " ' fell ',\n",
              " ' hard ',\n",
              " ', ',\n",
              " ' you ',\n",
              " ' took ',\n",
              " ' a ',\n",
              " ' step ',\n",
              " ' back ',\n",
              " '\\ue302 without ',\n",
              " ' me ',\n",
              " ', ',\n",
              " ' without ',\n",
              " ' me ',\n",
              " '\\ue302 without ',\n",
              " ' me ',\n",
              " '-',\n",
              " ' e ',\n",
              " '-',\n",
              " ' e ',\n",
              " '-',\n",
              " ' e ',\n",
              " '-',\n",
              " ' e ',\n",
              " ' [',\n",
              " '\\ue302 pre ',\n",
              " '-',\n",
              " '\\ue302 chorus ',\n",
              " '] ',\n",
              " '\\ue302 and ',\n",
              " ' he ',\n",
              " \"'\",\n",
              " ' s ',\n",
              " ' long ',\n",
              " ' gone ',\n",
              " ' when ',\n",
              " ' he ',\n",
              " \"'\",\n",
              " ' s ',\n",
              " ' next ',\n",
              " ' to ',\n",
              " ' me ',\n",
              " '\\ue302 and ',\n",
              " ' I ',\n",
              " ' realize ',\n",
              " ' the ',\n",
              " ' blame ',\n",
              " ' is ',\n",
              " ' on ',\n",
              " ' me ',\n",
              " ' [',\n",
              " ' SEP ',\n",
              " '] ',\n",
              " ' [',\n",
              " '\\ue302 chorus ',\n",
              " '] ',\n",
              " \" '\",\n",
              " '\\ue302 cause ',\n",
              " ' I ',\n",
              " ' knew ',\n",
              " ' you ',\n",
              " ' were ',\n",
              " ' trouble ',\n",
              " ' when ',\n",
              " ' you ',\n",
              " ' walked ',\n",
              " ' in ',\n",
              " '\\ue302 so ',\n",
              " ' shame ',\n",
              " ' on ',\n",
              " ' me ',\n",
              " ' now ',\n",
              " '\\ue302 flew ',\n",
              " ' me ',\n",
              " ' to ',\n",
              " ' places ',\n",
              " ' I ',\n",
              " \"'\",\n",
              " ' d ',\n",
              " ' never ',\n",
              " ' been ',\n",
              " \" '\",\n",
              " '\\ue302 til ',\n",
              " ' you ',\n",
              " ' put ',\n",
              " ' me ',\n",
              " ' down ',\n",
              " ', ',\n",
              " ' oh ',\n",
              " ' I ',\n",
              " ' knew ',\n",
              " ' you ',\n",
              " ' were ',\n",
              " ' trouble ',\n",
              " ' when ',\n",
              " ' you ',\n",
              " ' walked ',\n",
              " ' in ',\n",
              " '\\ue302 so ',\n",
              " ' shame ',\n",
              " ' on ',\n",
              " ' me ',\n",
              " ' now ',\n",
              " '\\ue302 flew ',\n",
              " ' me ',\n",
              " ' to ',\n",
              " ' places ',\n",
              " ' I ',\n",
              " \"'\",\n",
              " ' d ',\n",
              " ' never ',\n",
              " ' been ',\n",
              " '\\ue302 now ',\n",
              " ', ',\n",
              " ' I ',\n",
              " \"'\",\n",
              " ' m ',\n",
              " ' lying ',\n",
              " ' on ',\n",
              " ' the ',\n",
              " ' cold ',\n",
              " ', ',\n",
              " ' hard ',\n",
              " ' ground ',\n",
              " ' [',\n",
              " ' SEP ',\n",
              " '] ',\n",
              " ' [',\n",
              " '\\ue302 post ',\n",
              " '-',\n",
              " '\\ue302 chorus ',\n",
              " '] ',\n",
              " '\\ue302 oh ',\n",
              " ', ',\n",
              " ' oh ',\n",
              " '-',\n",
              " ' oh ',\n",
              " '\\ue302 trouble ',\n",
              " ', ',\n",
              " ' trouble ',\n",
              " ', ',\n",
              " ' trouble ',\n",
              " '\\ue302 oh ',\n",
              " ', ',\n",
              " ' oh ',\n",
              " '-',\n",
              " ' oh ',\n",
              " '\\ue302 trouble ',\n",
              " ', ',\n",
              " ' trouble ',\n",
              " ', ',\n",
              " ' trouble ',\n",
              " ' [',\n",
              " ' SEP ',\n",
              " '] ',\n",
              " ' [',\n",
              " '\\ue302 verse ',\n",
              " ' 2 ',\n",
              " '] ',\n",
              " '\\ue302 no ',\n",
              " ' apologies ',\n",
              " ', ',\n",
              " ' he ',\n",
              " \"'\",\n",
              " ' ll ',\n",
              " ' never ',\n",
              " ' see ',\n",
              " ' you ',\n",
              " ' cry ',\n",
              " '\\ue302 pretends ',\n",
              " ' he ',\n",
              " ' doesn ',\n",
              " \"'\",\n",
              " ' t ',\n",
              " ' know ',\n",
              " ' that ',\n",
              " ' he ',\n",
              " \"'\",\n",
              " ' s ',\n",
              " ' the ',\n",
              " ' reason ',\n",
              " ' why ',\n",
              " '\\ue302 you ',\n",
              " \"'\",\n",
              " ' re ',\n",
              " ' drowning ',\n",
              " ', ',\n",
              " ' you ',\n",
              " \"'\",\n",
              " ' re ',\n",
              " ' drowning ',\n",
              " '\\ue302 you ',\n",
              " \"'\",\n",
              " ' re ',\n",
              " ' drowning ',\n",
              " '-',\n",
              " ' ing ',\n",
              " '-',\n",
              " ' ing ',\n",
              " '-',\n",
              " ' ing ',\n",
              " '-',\n",
              " ' ing ',\n",
              " '\\ue302 and ',\n",
              " ' I ',\n",
              " ' heard ',\n",
              " ' you ',\n",
              " ' moved ',\n",
              " ' on ',\n",
              " ' from ',\n",
              " ' whispers ',\n",
              " ' on ',\n",
              " ' the ',\n",
              " ' street ',\n",
              " ' A ',\n",
              " ' new ',\n",
              " ' notch ',\n",
              " ' in ',\n",
              " ' your ',\n",
              " ' belt ',\n",
              " ' is ',\n",
              " ' all ',\n",
              " ' I ',\n",
              " \"'\",\n",
              " ' ll ',\n",
              " ' ever ',\n",
              " ' be ',\n",
              " '\\ue302 and ',\n",
              " ' now ',\n",
              " ', ',\n",
              " ' I ',\n",
              " ' see ',\n",
              " ', ',\n",
              " ' now ',\n",
              " ', ',\n",
              " ' I ',\n",
              " ' see ',\n",
              " '\\ue302 now ',\n",
              " ', ',\n",
              " ' I ',\n",
              " ' see ',\n",
              " '-',\n",
              " ' e ',\n",
              " '-',\n",
              " ' e ',\n",
              " '-',\n",
              " ' e ',\n",
              " '-',\n",
              " ' e ',\n",
              " '\\ue302 you ',\n",
              " ' might ',\n",
              " ' also ',\n",
              " ' like ',\n",
              " '[',\n",
              " '\\ue302 pre ',\n",
              " '-',\n",
              " '\\ue302 chorus ',\n",
              " '] ',\n",
              " '\\ue302 he ',\n",
              " ' was ',\n",
              " ' long ',\n",
              " ' gone ',\n",
              " ' when ',\n",
              " ' he ',\n",
              " ' met ',\n",
              " ' me ',\n",
              " '\\ue302 and ',\n",
              " ' I ',\n",
              " ' realize ',\n",
              " ' the ',\n",
              " ' joke ',\n",
              " ' is ',\n",
              " ' on ',\n",
              " ' me ',\n",
              " ', ',\n",
              " ' yeah ',\n",
              " ' [',\n",
              " ' SEP ',\n",
              " '] ',\n",
              " ' [',\n",
              " '\\ue302 chorus ',\n",
              " '] ',\n",
              " ' I ',\n",
              " ' knew ',\n",
              " ' you ',\n",
              " ' were ',\n",
              " ' trouble ',\n",
              " ' when ',\n",
              " ' you ',\n",
              " ' walked ',\n",
              " ' in ',\n",
              " ' (',\n",
              " '\\ue302 oh ',\n",
              " ') ',\n",
              " '\\ue302 so ',\n",
              " ' shame ',\n",
              " ' on ',\n",
              " ' me ',\n",
              " ' now ',\n",
              " '\\ue302 flew ',\n",
              " ' me ',\n",
              " ' to ',\n",
              " ' places ',\n",
              " ' I ',\n",
              " \"'\",\n",
              " ' d ',\n",
              " ' never ',\n",
              " ' been ',\n",
              " \" '\",\n",
              " '\\ue302 til ',\n",
              " ' you ',\n",
              " ' put ',\n",
              " ' me ',\n",
              " ' down ',\n",
              " ', ',\n",
              " ' oh ',\n",
              " ' I ',\n",
              " ' knew ',\n",
              " ' you ',\n",
              " ' were ',\n",
              " ' trouble ',\n",
              " ' when ',\n",
              " ' you ',\n",
              " ' walked ',\n",
              " ' in ',\n",
              " '\\ue302 so ',\n",
              " ' shame ',\n",
              " ' on ',\n",
              " ' me ',\n",
              " ' now ',\n",
              " '\\ue302 flew ',\n",
              " ' me ',\n",
              " ' to ',\n",
              " ' places ',\n",
              " ' I ',\n",
              " \"'\",\n",
              " ' d ',\n",
              " ' never ',\n",
              " ' been ',\n",
              " ' (',\n",
              " '\\ue302 yeah ',\n",
              " ') ',\n",
              " '\\ue302 now ',\n",
              " ', ',\n",
              " ' I ',\n",
              " \"'\",\n",
              " ' m ',\n",
              " ' lying ',\n",
              " ' on ',\n",
              " ' the ',\n",
              " ' cold ',\n",
              " ', ',\n",
              " ' hard ',\n",
              " ' ground ',\n",
              " ' [',\n",
              " ' SEP ',\n",
              " '] ',\n",
              " ' [',\n",
              " '\\ue302 post ',\n",
              " '-',\n",
              " '\\ue302 chorus ',\n",
              " '] ',\n",
              " '\\ue302 oh ',\n",
              " ', ',\n",
              " ' oh ',\n",
              " '-',\n",
              " ' oh ',\n",
              " '\\ue302 trouble ',\n",
              " ', ',\n",
              " ' trouble ',\n",
              " ', ',\n",
              " ' trouble ',\n",
              " ' (',\n",
              " '\\ue302 yeah ',\n",
              " ', ',\n",
              " ' trouble ',\n",
              " ') ',\n",
              " '\\ue302 oh ',\n",
              " ', ',\n",
              " ' oh ',\n",
              " '-',\n",
              " ' oh ',\n",
              " '\\ue302 trouble ',\n",
              " ', ',\n",
              " ' trouble ',\n",
              " ', ',\n",
              " ' trouble ',\n",
              " ' [',\n",
              " ' SEP ',\n",
              " '] ',\n",
              " ' [',\n",
              " '\\ue302 bridge ',\n",
              " '] ',\n",
              " '\\ue302 and ',\n",
              " ' the ',\n",
              " ' saddest ',\n",
              " ' fear ',\n",
              " '\\ue302 comes ',\n",
              " ' creeping ',\n",
              " ' in ',\n",
              " '\\ue302 that ',\n",
              " ' you ',\n",
              " ' never ',\n",
              " ' loved ',\n",
              " ' me ',\n",
              " '\\ue302 or ',\n",
              " ' her ',\n",
              " ', ',\n",
              " ' or ',\n",
              " ' anyone ',\n",
              " ', ',\n",
              " ' or ',\n",
              " ' anything ',\n",
              " ', ',\n",
              " ' yeah ',\n",
              " ' [',\n",
              " '\\ue302 chorus ',\n",
              " '] ',\n",
              " ' I ',\n",
              " ' knew ',\n",
              " ' you ',\n",
              " ' were ',\n",
              " ' trouble ',\n",
              " ' when ',\n",
              " ' you ',\n",
              " ' walked ',\n",
              " ' in ',\n",
              " '\\ue302 so ',\n",
              " ' shame ',\n",
              " ' on ',\n",
              " ' me ',\n",
              " ' now ',\n",
              " '\\ue302 flew ',\n",
              " ' me ',\n",
              " ' to ',\n",
              " ' places ',\n",
              " ' I ',\n",
              " \"'\",\n",
              " ' d ',\n",
              " ' never ',\n",
              " ' been ',\n",
              " ' (',\n",
              " '\\ue302 never ',\n",
              " ' been ',\n",
              " ') ',\n",
              " \" '\",\n",
              " '\\ue302 til ',\n",
              " ' you ',\n",
              " ' put ',\n",
              " ' me ',\n",
              " ' down ',\n",
              " ', ',\n",
              " ' oh ',\n",
              " ' I ',\n",
              " ' knew ',\n",
              " ' you ',\n",
              " ' were ',\n",
              " ' trouble ',\n",
              " ' when ',\n",
              " ' you ',\n",
              " ' walked ',\n",
              " ' in ',\n",
              " ' (',\n",
              " '\\ue302 knew ',\n",
              " ' it ',\n",
              " ' right ',\n",
              " ' there ',\n",
              " ') ',\n",
              " '\\ue302 so ',\n",
              " ' shame ',\n",
              " ' on ',\n",
              " ' me ',\n",
              " ' now ',\n",
              " ' (',\n",
              " '\\ue302 knew ',\n",
              " ' it ',\n",
              " ' right ',\n",
              " ' there ',\n",
              " ') ',\n",
              " '\\ue302 flew ',\n",
              " ' me ',\n",
              " ' to ',\n",
              " ' places ',\n",
              " ' I ',\n",
              " \"'\",\n",
              " ' d ',\n",
              " ' never ',\n",
              " ' been ',\n",
              " ' (',\n",
              " '\\ue302 ooh ',\n",
              " ') ',\n",
              " '\\ue302 now ',\n",
              " ', ',\n",
              " ' I ',\n",
              " \"'\",\n",
              " ' m ',\n",
              " ' lying ',\n",
              " ' on ',\n",
              " ' the ',\n",
              " ' cold ',\n",
              " ', ',\n",
              " ' hard ',\n",
              " ' ground ',\n",
              " ' [',\n",
              " ' SEP ',\n",
              " '] ',\n",
              " ' [',\n",
              " '\\ue302 post ',\n",
              " '-',\n",
              " '\\ue302 chorus ',\n",
              " '] ',\n",
              " '\\ue302 oh ',\n",
              " ', ',\n",
              " ' oh ',\n",
              " '-',\n",
              " ' oh ',\n",
              " '\\ue302 trouble ',\n",
              " ', ',\n",
              " ' trouble ',\n",
              " ', ',\n",
              " ' trouble ',\n",
              " ' (',\n",
              " '\\ue302 oh ',\n",
              " ') ',\n",
              " '\\ue302 oh ',\n",
              " ', ',\n",
              " ' oh ',\n",
              " '-',\n",
              " ' oh ',\n",
              " '\\ue302 trouble ',\n",
              " ', ',\n",
              " ' trouble ',\n",
              " ', ',\n",
              " ' trouble ',\n",
              " ' I ',\n",
              " ' knew ',\n",
              " ' you ',\n",
              " ' were ',\n",
              " ' trouble ',\n",
              " ' when ',\n",
              " ' you ',\n",
              " ' walked ',\n",
              " ' in ',\n",
              " '\\ue302 trouble ',\n",
              " ', ',\n",
              " ' trouble ',\n",
              " ', ',\n",
              " ' trouble ',\n",
              " ' I ',\n",
              " ' knew ',\n",
              " ' you ',\n",
              " ' were ',\n",
              " ' trouble ',\n",
              " ' when ',\n",
              " ' you ',\n",
              " ' walked ',\n",
              " ' in ',\n",
              " '\\ue302 trouble ',\n",
              " ', ',\n",
              " ' trouble ',\n",
              " ', ',\n",
              " ' trouble ',\n",
              " ' [',\n",
              " '\\ue302 outro ',\n",
              " ']',\n",
              " ' 112',\n",
              " '\\ue302 embed ',\n",
              " ' [',\n",
              " ' EOS ',\n",
              " '] ']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer('subword')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, map(lambda x:x.strip(),train_iter)), specials=[\"[UNK]\", \"[SEP]\", \"[EOS]\"], max_tokens=15346)\n",
        "vocab.set_default_index(vocab[\"[UNK]\"])\n",
        "\n",
        "def data_process(raw_text_iter):\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item.strip())), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "train_data = data_process(train_iter)\n",
        "test_data = data_process(test_iter)\n",
        "val_data = data_process(val_iter)"
      ],
      "metadata": {
        "id": "WzX_03cVyp0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(data, seq_len):\n",
        "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Arguments:\n",
        "        data: Tensor, shape ``[N]``\n",
        "        seq_len: int, sequence length\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape ``[seq_len, seq_len // bsz]``\n",
        "    \"\"\"\n",
        "    bsz = data.size(0) // seq_len\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "# Set to 5 for keeping\n",
        "seq_len = 256\n",
        "\n",
        "train_data = batchify(train_data, seq_len)\n",
        "val_data = batchify(val_data, seq_len)\n",
        "test_data = batchify(test_data, seq_len)"
      ],
      "metadata": {
        "id": "Sy8vxVm7y-Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIfMZxGN0DnV",
        "outputId": "c1a4e889-6140-4c02-f4e9-661d2981769d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([256, 996])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are generating source and target vector\n",
        "Target is the right shifted version of source text\n",
        "We are going to generate the text in Seq2Seq manner\n",
        "For this we will predict the next word based on the left context\n",
        "which is also known as Causal Language modelling\n"
      ],
      "metadata": {
        "id": "f9RvkHdW2Ma2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bptt = 35\n",
        "def get_batch(source, i):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
        "        target has shape ``[seq_len * batch_size]``\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "R4-g7dFnznCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling\n",
        "\n",
        "We are goint to use a custom transformer language model to generate lyrics"
      ],
      "metadata": {
        "id": "1MjUDQsNztH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        # Positional encoding calculation\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "\n",
        "        # Positional encoding layer\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        # Transformer encoder layers\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "\n",
        "        # Word embedding layer\n",
        "        self.encoder = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Linear decoder layer\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            src: Tensor, shape ``[seq_len, batch_size]``\n",
        "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
        "        \"\"\"\n",
        "        # Generating embeddings from encoder and normalizing it with model dimensions\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        # Encoding position in the text embeddings\n",
        "        src = self.pos_encoder(src)\n",
        "        # Transformer encoder\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        # Linear decoder\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "      for _ in range(max_new_tokens):\n",
        "        logits = self(idx)\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "      return idx\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int):\n",
        "    \"\"\"Generates an upper-triangular matrix of ``-inf``, with zeros on ``diag``.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
      ],
      "metadata": {
        "id": "wnHOhFLYzrze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Config is a dictionary that contains hyperparameters needed for training"
      ],
      "metadata": {
        "id": "B9P8ZqRf5G-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"ntoken\": len_vocab,\n",
        "    \"d_model\": 500,\n",
        "    \"d_hid\": 500,\n",
        "    \"nlayers\": 2,\n",
        "    \"nhead\": 2,\n",
        "    \"dropout\": 0.2,\n",
        "    \"lr\": 5.0\n",
        "}\n",
        "\n",
        "\n",
        "model = TransformerModel(\n",
        "    ntoken = config[\"ntoken\"],\n",
        "    d_model = config[\"d_model\"],\n",
        "    nhead = config[\"nhead\"],\n",
        "    d_hid = config[\"d_hid\"],\n",
        "    nlayers = config[\"nlayers\"],\n",
        "    dropout = config[\"dropout\"]\n",
        "    ).to(device)"
      ],
      "metadata": {
        "id": "-vxgWE-J3a_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "zUbvUcIE5QDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = config[\"lr\"]  # learning rate\n",
        "# Define the optimizer using stochastic gradient descent (SGD)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "# Define a learning rate scheduler that decreases the learning rate over time\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model):\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "\n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        output = model(data)\n",
        "        output_flat = output.view(-1, config[\"ntoken\"])\n",
        "        loss = criterion(output_flat, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradients to prevent exploding gradients problem\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            # Calculate perplexity\n",
        "            ppl = math.exp(cur_loss)\n",
        "\n",
        "            # Print the training progress and metrics\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model, eval_data):\n",
        "    model.eval()  # Turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            seq_len = data.size(0)\n",
        "\n",
        "            output = model(data)\n",
        "            output_flat = output.view(-1, config[\"ntoken\"])\n",
        "\n",
        "            # Compute the loss and accumulate it\n",
        "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
        "    # Return the average loss over the evaluation data\n",
        "    return total_loss / (len(eval_data) - 1)"
      ],
      "metadata": {
        "id": "0d8SJpGE4mkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 20\n",
        "\n",
        "best_model_params_path = os.path.join(\"/content/drive/MyDrive/Colab_Notebooks/Outsystems/RNN_for_lang_modelling/saved_models/model.pth\")\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "  epoch_start_time = time.time()\n",
        "  train(model)\n",
        "  val_loss = evaluate(model, val_data)\n",
        "  val_ppl = math.exp(val_loss)\n",
        "  elapsed = time.time() - epoch_start_time\n",
        "  print('-' * 89)\n",
        "  print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "        f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "  print('-' * 89)\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "  scheduler.step()\n",
        "  model.load_state_dict(torch.load(best_model_params_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eGVAA6c59q0",
        "outputId": "25f37ef3-02ea-41b4-abba-cd22edd8bdce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  6.74s | valid loss 22.67 | valid ppl 6993987391.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  6.12s | valid loss  7.54 | valid ppl  1882.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time:  6.14s | valid loss  7.00 | valid ppl  1098.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time:  6.24s | valid loss  6.73 | valid ppl   840.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time:  6.28s | valid loss  7.86 | valid ppl  2598.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time:  6.33s | valid loss  8.49 | valid ppl  4871.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time:  6.37s | valid loss  8.08 | valid ppl  3230.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time:  6.39s | valid loss  6.41 | valid ppl   605.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time:  6.39s | valid loss  5.70 | valid ppl   297.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time:  6.36s | valid loss  6.11 | valid ppl   449.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time:  6.32s | valid loss  5.47 | valid ppl   237.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time:  6.30s | valid loss  5.01 | valid ppl   150.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time:  6.26s | valid loss  4.82 | valid ppl   123.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time:  6.24s | valid loss  4.31 | valid ppl    74.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time:  6.27s | valid loss  3.78 | valid ppl    43.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time:  6.26s | valid loss  3.59 | valid ppl    36.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time:  6.25s | valid loss  3.24 | valid ppl    25.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time:  6.28s | valid loss  3.18 | valid ppl    24.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time:  6.32s | valid loss  3.05 | valid ppl    21.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time:  6.33s | valid loss  2.76 | valid ppl    15.78\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "4rMK_f6D7PZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/Colab_Notebooks/Outsystems/RNN_for_lang_modelling/saved_models/model.pth\"\n",
        "model = TransformerModel(\n",
        "    ntoken = config[\"ntoken\"],\n",
        "    d_model = config[\"d_model\"],\n",
        "    nhead = config[\"nhead\"],\n",
        "    d_hid = config[\"d_hid\"],\n",
        "    nlayers = config[\"nlayers\"],\n",
        "    dropout = config[\"dropout\"]\n",
        "    ).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  model.load_state_dict(torch.load(save_path))"
      ],
      "metadata": {
        "id": "Jkw55YMa7RMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = tokenizer(test[1][:70].strip())\n",
        "# context = torch.tensor([vocab[item] for item in prompt], dtype=torch.long).unsqueeze(0).cuda()\n",
        "# itos = vocab.get_itos()\n",
        "# decode = lambda l: ''.join([itos[i] for i in l])\n",
        "# print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "id": "OP7JZ4gGWLCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt, max_seq_len, temperature, model, tokenizer, vocab, device=device, beam_width=4, seed=0):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    # Set the random seed if provided\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    # Tokenize the prompt and convert to indices using the vocabulary\n",
        "    tokens = tokenizer(prompt.strip())\n",
        "    prompt_indices = [vocab[t] for t in tokens]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        beam = [(prompt_indices, 0.0)] # Initialize the beam with the prompt\n",
        "        completed_sequences = [] # Store completed sequences\n",
        "\n",
        "        for _ in range(max_seq_len):\n",
        "            candidates = [] # Store candidate sequences for the next step\n",
        "\n",
        "            # Expand the beam by generating new candidates\n",
        "            for seq_indices, seq_score in beam:\n",
        "                input_tensor = torch.LongTensor(seq_indices).unsqueeze(1).to(device)\n",
        "                output = model(input_tensor)\n",
        "                logits = output[-1, -1, :]\n",
        "\n",
        "                probs = torch.softmax(logits / temperature, dim=-1)\n",
        "                topk_probs, topk_indices = torch.topk(probs, beam_width)\n",
        "\n",
        "                # Generate new candidates based on top-k probabilities\n",
        "                for prob, index in zip(topk_probs.squeeze(), topk_indices.squeeze()):\n",
        "                    new_seq_indices = seq_indices + [index.item()]\n",
        "                    new_seq_score = seq_score - torch.log(prob).item()\n",
        "                    candidates.append((new_seq_indices, new_seq_score))\n",
        "\n",
        "            # Select top-k candidates for the next iteration\n",
        "            candidates = sorted(candidates, key=lambda x: x[1])[:beam_width]\n",
        "            beam = []\n",
        "\n",
        "            # Check if any candidates have completed sequences\n",
        "            for candidate_indices, candidate_score in candidates:\n",
        "                if candidate_indices[-1] == vocab[\"[EOS]\"]:\n",
        "                    completed_sequences.append((candidate_indices, candidate_score))\n",
        "                else:\n",
        "                    beam.append((candidate_indices, candidate_score))\n",
        "\n",
        "            # Break the loop if enough completed sequences have been found\n",
        "            if len(completed_sequences) >= beam_width:\n",
        "                break\n",
        "    try:\n",
        "      # Select the best completed sequence with the lowest score\n",
        "      best_sequence_indices, _ = min(completed_sequences, key=lambda x: x[1])\n",
        "    except ValueError:\n",
        "      # If no completed sequences are found, select the best candidate from the beam\n",
        "      best_sequence_indices, _ = min(beam, key=lambda x: x[1])\n",
        "\n",
        "    # Convert the sequence indices back to tokens\n",
        "    itos = vocab.get_itos()\n",
        "    generated_text = [itos[i] for i in best_sequence_indices]\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "ZU3V87XI7WnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = test[1][:70]\n",
        "max_seq_len = 100\n",
        "\n",
        "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
        "for temperature in temperatures:\n",
        "  generation = generate_text(prompt, max_seq_len, temperature, model, tokenizer, vocab)\n",
        "  print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KaXXnm78YqG",
        "outputId": "ae2665c1-1102-4f83-dcbe-a75434eda53f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n",
            " how  '  s   one   to   know  ?   I  '  d   meet   you   where   the   spirit   meets   the   bones   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a \n",
            "\n",
            "0.7\n",
            " how  '  s   one   to   know  ?   I  '  d   meet   you   where   the   spirit   meets   the   bones   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a \n",
            "\n",
            "0.75\n",
            " how  '  s   one   to   know  ?   I  '  d   meet   you   where   the   spirit   meets   the   bones   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a \n",
            "\n",
            "0.8\n",
            " how  '  s   one   to   know  ?   I  '  d   meet   you   where   the   spirit   meets   the   bones   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a \n",
            "\n",
            "1.0\n",
            " how  '  s   one   to   know  ?   I  '  d   meet   you   where   the   spirit   meets   the   bones   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a   in   a \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OU5OzWg7ubIB",
        "outputId": "d7f36ac3-b7d6-4ef4-a428-9fc1db12a5b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"How's one to know? I'd meet you where the spirit meets the bones In a \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to limited vocabulary in training and less data, the generated text is not cohesive. This is also corroborated by high perplexity (57.50). However, we can tackle this problem by data augmentation or by using a task agnostic pre-trained language model. In the second notebook \"GPT2_for_LM\" we use finetune a GPT2 model on the text.txt file to generate song lyrics"
      ],
      "metadata": {
        "id": "9WfZ1RM-Hjk0"
      }
    }
  ]
}